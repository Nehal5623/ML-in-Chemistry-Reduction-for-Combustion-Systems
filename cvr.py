# -*- coding: utf-8 -*-
"""CVR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Nq6qySmZeVghT9o422uN9GPqiCF6V4A
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import joblib
import ipywidgets as widgets
from IPython.display import display, Markdown
from google.colab import drive
import os

drive.mount('/content/drive')


file_path = '/content/drive/MyDrive/BTP_ETE/CV.csv'


data = pd.read_csv(file_path)

# Input features: Temperature, Pressure, Phi, Volume
temperature = data['T0'].values
pressure = data['P0'].values
phi = data['phi'].values
volume = data['volume'].values
X = np.column_stack((temperature, pressure, phi, volume))

# Output species
Y_raw = data.iloc[:, 6:].values
species_names = data.columns[6:]

# Handle missing data
imputer = SimpleImputer(strategy='mean')
Y = imputer.fit_transform(Y_raw)

# Widgets for hyperparameters
lr_slider = widgets.FloatSlider(value=0.05, min=0.01, max=0.3, step=0.01, description="Learning R")
n_estimators_slider = widgets.IntSlider(value=800, min=100, max=1500, step=50, description="Max Iter")
random_state_slider = widgets.IntSlider(value=42, min=0, max=100, step=1, description="Random St.")
pca_slider = widgets.IntSlider(value=2, min=1, max=Y.shape[1], step=1, description="PCA Comp.")
max_depth_slider = widgets.IntSlider(value=7, min=1, max=20, step=1, description="Max Depth")
min_samples_slider = widgets.IntSlider(value=10, min=2, max=100, step=1, description="Min Sample")
max_bins_slider = widgets.IntSlider(value=255, min=64, max=255, step=1, description="Max Bins")
l2_slider = widgets.FloatSlider(value=0.0, min=0.0, max=1.0, step=0.01, description="L2 Reg.")

# Widgets for user input (prediction)
temp_input = widgets.FloatText(value=1200, description="Temp (K):")
press_input = widgets.FloatText(value=1.0, description="Pressure (atm):")
phi_input = widgets.FloatText(value=1.0, description="Phi (ϕ):")
vol_input = widgets.FloatText(value=1.0, description="Volume (m³):")

# Buttons
train_button = widgets.Button(description="Train Model", button_style='success')
predict_button = widgets.Button(description="Predict Top 5 Species", button_style='info')

output = widgets.Output()
trained_model = {"model": None}

def train_model(_):
    output.clear_output(wait=True)
    with output:
        try:
            scaler_X = StandardScaler()
            scaler_Y = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)
            Y_scaled = scaler_Y.fit_transform(Y)

            pca = PCA(n_components=pca_slider.value)
            Y_pca = pca.fit_transform(Y_scaled)

            X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_pca, test_size=0.3, random_state=random_state_slider.value)
            X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=random_state_slider.value)

            model = MultiOutputRegressor(HistGradientBoostingRegressor(
                learning_rate=lr_slider.value,
                max_iter=n_estimators_slider.value,
                max_depth=max_depth_slider.value,
                min_samples_leaf=min_samples_slider.value,
                max_bins=max_bins_slider.value,
                l2_regularization=l2_slider.value,
                random_state=random_state_slider.value
            ))
            model.fit(X_train, Y_train)

            train_pred = model.predict(X_train)
            val_pred = model.predict(X_val)
            test_pred = model.predict(X_test)

            train_mse = mean_squared_error(Y_train, train_pred)
            val_mse = mean_squared_error(Y_val, val_pred)
            test_mse = mean_squared_error(Y_test, test_pred)

            train_r2 = r2_score(Y_train, train_pred)
            val_r2 = r2_score(Y_val, val_pred)
            test_r2 = r2_score(Y_test, test_pred)

            # Save model
            save_path = "/content/drive/MyDrive/BTP/gradient_boosting_model_new_input.pkl"
            joblib.dump((pca, scaler_X, scaler_Y, model), save_path)
            trained_model["model"] = (pca, scaler_X, scaler_Y, model)

            print(f"Training with: LR={lr_slider.value}, Iter={n_estimators_slider.value}, PCA={pca_slider.value}")
            print(f"Data split: Train={len(X_train)} | Val={len(X_val)} | Test={len(X_test)}")
            print(f"\n Training MSE: {train_mse:.5f} | R²: {train_r2:.5f}")
            print(f"Validation MSE: {val_mse:.5f} | R²: {val_r2:.5f}")
            print(f"Test MSE: {test_mse:.5f} | R²: {test_r2:.5f}")
            print(f"Model saved to: {save_path}")
        except Exception as e:
            print("Training failed:", e)

def predict_species(_):
    with output:
        if trained_model["model"] is None:
            print("Please train the model first.")
            return
        try:
            temp = temp_input.value
            press = press_input.value
            phi_val = phi_input.value
            vol = vol_input.value
            input_vals = np.array([[temp, press, phi_val, vol]])

            pca, scaler_X, scaler_Y, model = trained_model["model"]
            x_scaled = scaler_X.transform(input_vals)
            y_pred_pca = model.predict(x_scaled)
            y_scaled = pca.inverse_transform(y_pred_pca)
            y_pred = scaler_Y.inverse_transform(y_scaled)

            top5_idx = np.argsort(y_pred[0])[-5:][::-1]
            display(Markdown("### Top 5 Species by Mole Fraction:"))
            for i in top5_idx:
                print(f"{species_names[i]}: {y_pred[0][i]:.6f}")
        except Exception as e:
            print("Prediction failed:", e)

# Connect buttons
train_button.on_click(train_model)
predict_button.on_click(predict_species)

# Display UI
slider_box = widgets.VBox([
    lr_slider, n_estimators_slider, random_state_slider, pca_slider,
    max_depth_slider, min_samples_slider, max_bins_slider, l2_slider
])

input_box = widgets.VBox([
    temp_input, press_input, phi_input, vol_input
])

button_box = widgets.HBox([train_button, predict_button])

display(slider_box, input_box, button_box, output)

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import joblib
import ipywidgets as widgets
from IPython.display import display, Markdown
from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')


file_path = '/content/drive/MyDrive/BTP_ETE/CV.csv'

# Load data
data = pd.read_csv(file_path)

# Input features: Temperature, Pressure, Phi, Volume
temperature = data['T0'].values
pressure = data['P0'].values
phi = data['phi'].values
volume = data['volume'].values
X = np.column_stack((temperature, pressure, phi, volume))

# Output species (everything after Volume column)
Y_raw = data.iloc[:, 6:].values
species_names = data.columns[6:]

# Handle missing data
imputer = SimpleImputer(strategy='mean')
Y = imputer.fit_transform(Y_raw)

# Widgets for hyperparameters
lr_slider = widgets.FloatSlider(value=0.15, min=0.01, max=0.5, step=0.01, description="Learning R")
n_estimators_slider = widgets.IntSlider(value=500, min=100, max=1500, step=50, description="Max Iter")
random_state_slider = widgets.IntSlider(value=42, min=0, max=100, step=1, description="Random St.")
pca_slider = widgets.IntSlider(value=2, min=1, max=min(30,Y.shape[1]), step=1, description="PCA Comp.")
max_depth_slider = widgets.IntSlider(value=5, min=1, max=20, step=1, description="Max Depth")
min_samples_slider = widgets.IntSlider(value=20, min=2, max=100, step=1, description="Min Sample")
max_bins_slider = widgets.IntSlider(value=255, min=64, max=255, step=1, description="Max Bins")
l2_slider = widgets.FloatSlider(value=0.0, min=0.0, max=1.0, step=0.01, description="L2 Reg.")

# Widgets for user input (prediction)
temp_input = widgets.FloatText(value=1600, description="Temp (K):")
press_input = widgets.FloatText(value=2.0, description="Pressure (atm):")
phi_input = widgets.FloatText(value=0.5, description="Phi (ϕ):")
vol_input = widgets.FloatText(value=1.5, description="Volume (m³):")

# Buttons
train_button = widgets.Button(description="Train Model", button_style='success')
predict_button = widgets.Button(description="Predict Top 5 Species", button_style='info')

output = widgets.Output()
trained_model = {"model": None}

def train_model(_):
    output.clear_output(wait=True)
    with output:
        try:
            scaler_X = StandardScaler()
            scaler_Y = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)
            Y_scaled = scaler_Y.fit_transform(Y)

            pca = PCA(n_components=pca_slider.value)
            Y_pca = pca.fit_transform(Y_scaled)

            X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_pca, test_size=0.3, random_state=random_state_slider.value)
            X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=random_state_slider.value)

            model = MultiOutputRegressor(HistGradientBoostingRegressor(
                learning_rate=lr_slider.value,
                max_iter=n_estimators_slider.value,
                max_depth=max_depth_slider.value,
                min_samples_leaf=min_samples_slider.value,
                max_bins=max_bins_slider.value,
                l2_regularization=l2_slider.value,
                random_state=random_state_slider.value
            ))
            model.fit(X_train, Y_train)

            train_pred = model.predict(X_train)
            val_pred = model.predict(X_val)
            test_pred = model.predict(X_test)

            train_mse = mean_squared_error(Y_train, train_pred)
            val_mse = mean_squared_error(Y_val, val_pred)
            test_mse = mean_squared_error(Y_test, test_pred)

            train_r2 = r2_score(Y_train, train_pred)
            val_r2 = r2_score(Y_val, val_pred)
            test_r2 = r2_score(Y_test, test_pred)

            # Save model
            save_path = "/content/drive/MyDrive/BTP/gradient_boosting_model_new_input.pkl"
            joblib.dump((pca, scaler_X, scaler_Y, model), save_path)
            trained_model["model"] = (pca, scaler_X, scaler_Y, model)

            print(f"Training with: LR={lr_slider.value}, Iter={n_estimators_slider.value}, PCA={pca_slider.value}")
            print(f"Data split: Train={len(X_train)} | Val={len(X_val)} | Test={len(X_test)}")
            print(f"\n Training MSE: {train_mse:.5f} | R²: {train_r2:.5f}")
            print(f"Validation MSE: {val_mse:.5f} | R²: {val_r2:.5f}")
            print(f"Test MSE: {test_mse:.5f} | R²: {test_r2:.5f}")
            print(f"Model saved to: {save_path}")
        except Exception as e:
            print("Training failed:", e)

def predict_species(_):
    with output:
        if trained_model["model"] is None:
            print("Please train the model first.")
            return
        try:
            temp = temp_input.value
            press = press_input.value
            phi_val = phi_input.value
            vol = vol_input.value
            input_vals = np.array([[temp, press, phi_val, vol]])

            pca, scaler_X, scaler_Y, model = trained_model["model"]
            x_scaled = scaler_X.transform(input_vals)
            y_pred_pca = model.predict(x_scaled)
            y_scaled = pca.inverse_transform(y_pred_pca)
            y_pred = scaler_Y.inverse_transform(y_scaled)

            top5_idx = np.argsort(y_pred[0])[-5:][::-1]
            display(Markdown("### Top 5 Species by Mole Fraction:"))
            for i in top5_idx:
                print(f"{species_names[i]}: {y_pred[0][i]:.6f}")
        except Exception as e:
            print("Prediction failed:", e)

# Connect buttons
train_button.on_click(train_model)
predict_button.on_click(predict_species)

# Display UI
slider_box = widgets.VBox([
    lr_slider, n_estimators_slider, random_state_slider, pca_slider,
    max_depth_slider, min_samples_slider, max_bins_slider, l2_slider
])

input_box = widgets.VBox([
    temp_input, press_input, phi_input, vol_input
])

button_box = widgets.HBox([train_button, predict_button])

display(slider_box, input_box, button_box, output)

#Remove PCA or increase PCA to retain >95% variance.

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import joblib
import ipywidgets as widgets
from IPython.display import display, Markdown
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# File path
file_path = '/content/drive/MyDrive/BTP_ETE/CV.csv'

# Load data
data = pd.read_csv(file_path)

# Input features
temperature = data['T0'].values
pressure = data['P0'].values
phi = data['phi'].values
volume = data['volume'].values
X = np.column_stack((temperature, pressure, phi, volume))

# Output species (columns after volume)
Y_raw = data.iloc[:, 6:].values
species_names = data.columns[6:]

# Handle missing data
imputer = SimpleImputer(strategy='mean')
Y = imputer.fit_transform(Y_raw)

# Hyperparameter widgets
lr_slider = widgets.FloatSlider(value=0.02, min=0.01, max=0.3, step=0.01, description="Learning R")
n_estimators_slider = widgets.IntSlider(value=1200, min=100, max=1500, step=50, description="Max Iter")
random_state_slider = widgets.IntSlider(value=42, min=0, max=100, step=1, description="Random St.")
max_depth_slider = widgets.IntSlider(value=3, min=1, max=20, step=1, description="Max Depth")
min_samples_slider = widgets.IntSlider(value=30, min=2, max=100, step=1, description="Min Sample")
max_bins_slider = widgets.IntSlider(value=255, min=64, max=255, step=1, description="Max Bins")
l2_slider = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.01, description="L2 Reg.")
use_pca_checkbox = widgets.Checkbox(value=False, description="Use PCA (retain 90% variance)")

# Input widgets
temp_input = widgets.FloatText(value=1200, description="Temp (K):")
press_input = widgets.FloatText(value=1.0, description="Pressure (atm):")
phi_input = widgets.FloatText(value=1.0, description="Phi (ϕ):")
vol_input = widgets.FloatText(value=1.0, description="Volume (m³):")

# Buttons
train_button = widgets.Button(description="Train Model", button_style='success')
predict_button = widgets.Button(description="Predict Top 5 Species", button_style='info')
output = widgets.Output()

trained_model = {"model": None}

def train_model(_):
    output.clear_output(wait=True)
    with output:
        try:
            # Preprocessing
            scaler_X = StandardScaler()
            scaler_Y = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)
            Y_scaled = scaler_Y.fit_transform(Y)

            if use_pca_checkbox.value:
                pca = PCA(n_components=0.99)  # retain 90% variance
                Y_transformed = pca.fit_transform(Y_scaled)
                explained_var = pca.explained_variance_ratio_.sum()
                print(f"PCA Enabled: Retained Variance = {explained_var:.4f}")
            else:
                pca = None
                Y_transformed = Y_scaled
                print("PCA Disabled")

            # Train/val/test split
            X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_transformed, test_size=0.3, random_state=random_state_slider.value)
            X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=random_state_slider.value)

            # Model
            model = MultiOutputRegressor(HistGradientBoostingRegressor(
                learning_rate=lr_slider.value,
                max_iter=n_estimators_slider.value,
                max_depth=max_depth_slider.value,
                min_samples_leaf=min_samples_slider.value,
                max_bins=max_bins_slider.value,
                l2_regularization=l2_slider.value,
                random_state=random_state_slider.value
            ))
            model.fit(X_train, Y_train)

            # Predictions
            train_pred = model.predict(X_train)
            val_pred = model.predict(X_val)
            test_pred = model.predict(X_test)

            # Metrics
            train_mse = mean_squared_error(Y_train, train_pred)
            val_mse = mean_squared_error(Y_val, val_pred)
            test_mse = mean_squared_error(Y_test, test_pred)

            train_r2 = r2_score(Y_train, train_pred)
            val_r2 = r2_score(Y_val, val_pred)
            test_r2 = r2_score(Y_test, test_pred)

            # Save model
            save_path = "/content/drive/MyDrive/BTP/gradient_boosting_model_final.pkl"
            joblib.dump((pca, scaler_X, scaler_Y, model), save_path)
            trained_model["model"] = (pca, scaler_X, scaler_Y, model)

            # Output
            print(f"Training with: LR={lr_slider.value}, Iter={n_estimators_slider.value}")
            print(f"Data split: Train={len(X_train)} | Val={len(X_val)} | Test={len(X_test)}")
            print(f"\nTraining MSE: {train_mse:.5f} | R²: {train_r2:.5f}")
            print(f"Validation MSE: {val_mse:.5f} | R²: {val_r2:.5f}")
            print(f"Test MSE: {test_mse:.5f} | R²: {test_r2:.5f}")
            print(f"Model saved to: {save_path}")
        except Exception as e:
            print("Training failed:", e)

def predict_species(_):
    with output:
        if trained_model["model"] is None:
            print("Please train the model first.")
            return
        try:
            # User input
            temp = temp_input.value
            press = press_input.value
            phi_val = phi_input.value
            vol = vol_input.value
            input_vals = np.array([[temp, press, phi_val, vol]])

            # Prediction pipeline
            pca, scaler_X, scaler_Y, model = trained_model["model"]
            x_scaled = scaler_X.transform(input_vals)
            y_pred_pca = model.predict(x_scaled)

            if pca:
                y_scaled = pca.inverse_transform(y_pred_pca)
            else:
                y_scaled = y_pred_pca

            y_pred = scaler_Y.inverse_transform(y_scaled)

            # Top 5 species
            top5_idx = np.argsort(y_pred[0])[-5:][::-1]
            display(Markdown("### Top 5 Species by Mole Fraction:"))
            for i in top5_idx:
                print(f"{species_names[i]}: {y_pred[0][i]:.6f}")
        except Exception as e:
            print("Prediction failed:", e)

# Button triggers
train_button.on_click(train_model)
predict_button.on_click(predict_species)

# UI layout
slider_box = widgets.VBox([
    use_pca_checkbox,
    lr_slider, n_estimators_slider, random_state_slider,
    max_depth_slider, min_samples_slider, max_bins_slider, l2_slider
])

input_box = widgets.VBox([
    temp_input, press_input, phi_input, vol_input
])

button_box = widgets.HBox([train_button, predict_button])

display(slider_box, input_box, button_box, output)

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
from google.colab import drive
import joblib

# Mount Google Drive
drive.mount('/content/drive')

# Load dataset
file_path = '/content/drive/MyDrive/BTP_ETE/CV.csv'
data = pd.read_csv(file_path)

# Feature and target selection
X = data[['T0', 'P0', 'phi', 'volume']].values
Y_raw = data.iloc[:, 6:].values
species_names = data.columns[6:]

# Handle missing values
imputer = SimpleImputer(strategy='mean')
Y = imputer.fit_transform(Y_raw)

# Standardize inputs and outputs
scaler_X = StandardScaler()
scaler_Y = StandardScaler()
X_scaled = scaler_X.fit_transform(X)
Y_scaled = scaler_Y.fit_transform(Y)

# Apply PCA to retain >99% variance
pca = PCA(n_components=0.99)
Y_pca = pca.fit_transform(Y_scaled)
print(f"PCA Retained Variance: {np.sum(pca.explained_variance_ratio_):.4f}")
print(f"Reduced target dimensions: {Y_pca.shape[1]}")

# Split the data
X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_pca, test_size=0.3, random_state=42)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

# Define model for GridSearch
base_model = MultiOutputRegressor(HistGradientBoostingRegressor(random_state=42))

# Grid search parameters
param_grid = {
    'estimator__learning_rate': [0.01, 0.02, 0.05],
    'estimator__max_iter': [500, 800, 1200],
    'estimator__max_depth': [3, 5, 7],
    'estimator__min_samples_leaf': [10, 20, 50],
    'estimator__l2_regularization': [0.0, 0.2, 0.5]
}

# Grid search setup
grid_search = GridSearchCV(
    base_model,
    param_grid=param_grid,
    scoring='r2',
    cv=3,
    verbose=2,
    n_jobs=-1
)

# Train model
print("Running GridSearchCV...")
grid_search.fit(X_train, Y_train)

# Best estimator
best_model = grid_search.best_estimator_
print("\nBest Parameters:")
print(grid_search.best_params_)

# Evaluate model
def evaluate_model(name, X, Y, model):
    preds = model.predict(X)
    mse = mean_squared_error(Y, preds)
    r2 = r2_score(Y, preds)
    print(f"{name} MSE: {mse:.5f} | R²: {r2:.5f}")
    return mse, r2

print("\nModel Performance:")
evaluate_model("Training", X_train, Y_train, best_model)
evaluate_model("Validation", X_val, Y_val, best_model)
evaluate_model("Test", X_test, Y_test, best_model)

# Save model and scalers
model_path = '/content/drive/MyDrive/BTP/gridsearch_model_pca.pkl'
joblib.dump((pca, scaler_X, scaler_Y, best_model), model_path)
print(f"\nModel saved to: {model_path}")