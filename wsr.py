# -*- coding: utf-8 -*-
"""WSR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z1rFia0bdD8Wnwz6HhPwbGt7mzjPCztT
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import joblib
import ipywidgets as widgets
from IPython.display import display, Markdown
import os
from google.colab import drive

# Load data
drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/BTP_ETE/WSR.csv'
data = pd.read_csv(file_path)


X = data[['Time', 'Temperature']].values
species_names = data.columns[4:]
Y_raw = data.loc[:, species_names].values

# Handle missing data
imputer = SimpleImputer(strategy='mean')
Y = imputer.fit_transform(Y_raw)

# Widget controls
lr_slider = widgets.FloatSlider(value=0.15, min=0.01, max=0.5, step=0.01, description="Learning R")
n_estimators_slider = widgets.IntSlider(value=500, min=100, max=1000, step=50, description="Max Iter")
random_state_slider = widgets.IntSlider(value=42, min=0, max=100, step=1, description="Random St.")
pca_slider = widgets.IntSlider(value=10, min=1, max=30, step=1, description="PCA Comp.")
max_depth_slider = widgets.IntSlider(value=5, min=1, max=20, step=1, description="Max Depth")
min_samples_slider = widgets.IntSlider(value=20, min=2, max=100, step=1, description="Min Sample")
max_bins_slider = widgets.IntSlider(value=255, min=64, max=255, step=1, description="Max Bins")
l2_slider = widgets.FloatSlider(value=0.0, min=0.0, max=1.0, step=0.01, description="L2 Reg.")

time_input = widgets.FloatText(value=1e-9, description="Time (s):")
temp_input = widgets.FloatText(value=1500, description="Temp (K):")

train_button = widgets.Button(description="Train Model", button_style='success')
predict_button = widgets.Button(description="Predict Top 5 Species", button_style='info')

output = widgets.Output()
trained_model = {"model": None}

# Training function
def train_model(_):
    output.clear_output(wait=True)
    with output:
        try:
            scaler_X = StandardScaler()
            scaler_Y = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)
            Y_scaled = scaler_Y.fit_transform(Y)

            pca = PCA(n_components=pca_slider.value)
            Y_pca = pca.fit_transform(Y_scaled)

            X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_pca, test_size=0.3, random_state=random_state_slider.value)
            X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=random_state_slider.value)

            model = MultiOutputRegressor(HistGradientBoostingRegressor(
                learning_rate=lr_slider.value,
                max_iter=n_estimators_slider.value,
                max_depth=max_depth_slider.value,
                min_samples_leaf=min_samples_slider.value,
                max_bins=max_bins_slider.value,
                l2_regularization=l2_slider.value,
                random_state=random_state_slider.value
            ))
            model.fit(X_train, Y_train)

            train_pred = model.predict(X_train)
            val_pred = model.predict(X_val)
            test_pred = model.predict(X_test)

            train_mse = mean_squared_error(Y_train, train_pred)
            val_mse = mean_squared_error(Y_val, val_pred)
            test_mse = mean_squared_error(Y_test, test_pred)

            train_r2 = r2_score(Y_train, train_pred)
            val_r2 = r2_score(Y_val, val_pred)
            test_r2 = r2_score(Y_test, test_pred)

            joblib.dump((pca, scaler_X, scaler_Y, model), "wsr_model.pkl")
            trained_model["model"] = (pca, scaler_X, scaler_Y, model)

            print(f"\U0001F527 Training with: LR={lr_slider.value}, Iter={n_estimators_slider.value}, PCA={pca_slider.value}")
            print(f"\U0001F4CA Data split: Train={len(X_train)} | Val={len(X_val)} | Test={len(X_test)}")
            print(f"\nTraining MSE: {train_mse:.5e} | RÂ²: {train_r2:.5f}")
            print(f"Validation MSE: {val_mse:.5e} | RÂ²: {val_r2:.5f}")
            print(f"Test MSE: {test_mse:.5e} | RÂ²: {test_r2:.5f}")
            print("\U0001F4BE Model saved as 'wsr_model.pkl'")
        except Exception as e:
            print("Training failed:", e)

# Prediction function
def predict_species(_):
    with output:
        if trained_model["model"] is None:
            print("Please train the model first.")
            return
        try:
            time_val = time_input.value
            temp_val = temp_input.value
            input_vals = np.array([[time_val, temp_val]])

            pca, scaler_X, scaler_Y, model = trained_model["model"]
            x_scaled = scaler_X.transform(input_vals)
            y_pred_pca = model.predict(x_scaled)
            y_scaled = pca.inverse_transform(y_pred_pca)
            y_pred = scaler_Y.inverse_transform(y_scaled)

            top5_idx = np.argsort(y_pred[0])[-5:][::-1]

            display(Markdown("### Top 5 Species by Mole Fraction:"))
            for i in top5_idx:
                print(f"{species_names[i]}: {y_pred[0][i]:.6e}")
        except Exception as e:
            print("Prediction failed:", e)

# Widget bindings
train_button.on_click(train_model)
predict_button.on_click(predict_species)

slider_box = widgets.VBox([
    lr_slider, n_estimators_slider, random_state_slider, pca_slider,
    max_depth_slider, min_samples_slider, max_bins_slider, l2_slider
])

input_box = widgets.VBox([
    time_input, temp_input
])

button_box = widgets.HBox([train_button, predict_button])

display(slider_box, input_box, button_box, output)

"""##ANN Version"""

import pandas as pd
import numpy as np
import joblib
import os
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
import ipywidgets as widgets
from IPython.display import display, Markdown
from google.colab import drive

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/BTP_ETE/WSR.csv'
data = pd.read_csv(file_path)

#features and outputs
X = data[['Time', 'Temperature']].values
species_names = data.columns[4:]
Y_raw = data.loc[:, species_names].values

#Missing Value
imputer = SimpleImputer(strategy='mean')
Y = imputer.fit_transform(Y_raw)

# Widgets
lr_slider = widgets.FloatSlider(value=0.001, min=0.0001, max=0.01, step=0.0001, description="Learning Rate")
epochs_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description="Epochs")
batch_slider = widgets.IntSlider(value=32, min=8, max=128, step=8, description="Batch Size")
pca_slider = widgets.IntSlider(value=10, min=1, max=30, step=1, description="PCA Comp.")
random_state_slider = widgets.IntSlider(value=42, min=0, max=100, step=1, description="Random St.")

time_input = widgets.FloatText(value=1e-9, description="Time (s):")
temp_input = widgets.FloatText(value=1500, description="Temp (K):")

train_button = widgets.Button(description="Train Model", button_style='success')
predict_button = widgets.Button(description="Predict Top 5 Species", button_style='info')

output = widgets.Output()
trained_model = {"model": None}

# Train
def train_model(_):
    output.clear_output(wait=True)
    with output:
        try:
            # Preprocessing
            scaler_X = StandardScaler()
            scaler_Y = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)
            Y_scaled = scaler_Y.fit_transform(Y)

            pca = PCA(n_components=pca_slider.value)
            Y_pca = pca.fit_transform(Y_scaled)

            X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_pca, test_size=0.3, random_state=random_state_slider.value)
            X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=random_state_slider.value)

            # ANN Model
            model = Sequential([
                Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
                Dropout(0.2),
                Dense(64, activation='relu'),
                Dropout(0.2),
                Dense(Y_train.shape[1])
            ])

            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_slider.value),
                          loss='mse', metrics=['mse'])

            early_stop = EarlyStopping(patience=20, restore_best_weights=True)

            history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val),
                                epochs=epochs_slider.value, batch_size=batch_slider.value,
                                callbacks=[early_stop], verbose=0)

            # Predictions and evaluation
            train_pred = model.predict(X_train)
            val_pred = model.predict(X_val)
            test_pred = model.predict(X_test)

            train_mse = mean_squared_error(Y_train, train_pred)
            val_mse = mean_squared_error(Y_val, val_pred)
            test_mse = mean_squared_error(Y_test, test_pred)

            train_r2 = r2_score(Y_train, train_pred)
            val_r2 = r2_score(Y_val, val_pred)
            test_r2 = r2_score(Y_test, test_pred)

            model.save("wsr_ann_model.h5")
            joblib.dump((pca, scaler_X, scaler_Y), "wsr_ann_scalers.pkl")
            trained_model["model"] = (pca, scaler_X, scaler_Y, model)

            print(f"ðŸ”§ ANN Trained | LR={lr_slider.value} | Epochs={epochs_slider.value} | PCA={pca_slider.value}")
            print(f"ðŸ“Š Data split: Train={len(X_train)} | Val={len(X_val)} | Test={len(X_test)}")
            print(f"\nTraining MSE: {train_mse:.5e} | RÂ²: {train_r2:.5f}")
            print(f"Validation MSE: {val_mse:.5e} | RÂ²: {val_r2:.5f}")
            print(f"Test MSE: {test_mse:.5e} | RÂ²: {test_r2:.5f}")
            print("ðŸ’¾ Model saved as 'wsr_ann_model.h5'")
        except Exception as e:
            print("Training failed:", e)

# Prediction function
def predict_species(_):
    with output:
        if trained_model["model"] is None:
            print("Please train the model first.")
            return
        try:
            time_val = time_input.value
            temp_val = temp_input.value
            input_vals = np.array([[time_val, temp_val]])

            pca, scaler_X, scaler_Y, model = trained_model["model"]
            x_scaled = scaler_X.transform(input_vals)
            y_pred_pca = model.predict(x_scaled)
            y_scaled = pca.inverse_transform(y_pred_pca)
            y_pred = scaler_Y.inverse_transform(y_scaled)

            top5_idx = np.argsort(y_pred[0])[-5:][::-1]

            display(Markdown("### Top 5 Species by Mole Fraction:"))
            for i in top5_idx:
                print(f"{species_names[i]}: {y_pred[0][i]:.6e}")
        except Exception as e:
            print("Prediction failed:", e)

# Widget bindings
train_button.on_click(train_model)
predict_button.on_click(predict_species)

slider_box = widgets.VBox([
    lr_slider, epochs_slider, batch_slider, pca_slider, random_state_slider
])
input_box = widgets.VBox([time_input, temp_input])
button_box = widgets.HBox([train_button, predict_button])

display(slider_box, input_box, button_box, output)

"""Xgboost"""

import pandas as pd
import numpy as np
import joblib
import os
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error, r2_score
import ipywidgets as widgets
from IPython.display import display, Markdown
from google.colab import drive

import xgboost as xgb  # âœ… New import for XGBoost

# Mount Google Drive
drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/BTP_ETE/WSR.csv'
data = pd.read_csv(file_path)

# Extract features and outputs
X = data[['Time', 'Temperature']].values
species_names = data.columns[4:]
Y_raw = data.loc[:, species_names].values

# Handle missing values
imputer = SimpleImputer(strategy='mean')
Y = imputer.fit_transform(Y_raw)

# Widgets for hyperparameters
lr_slider = widgets.FloatSlider(value=0.01, min=0.001, max=0.3, step=0.001, description="Learning Rate")
trees_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description="Trees")
pca_slider = widgets.IntSlider(value=10, min=1, max=30, step=1, description="PCA Comp.")
random_state_slider = widgets.IntSlider(value=42, min=0, max=100, step=1, description="Random St.")

time_input = widgets.FloatText(value=1e-9, description="Time (s):")
temp_input = widgets.FloatText(value=1500, description="Temp (K):")

train_button = widgets.Button(description="Train Model", button_style='success')
predict_button = widgets.Button(description="Predict Top 5 Species", button_style='info')

output = widgets.Output()
trained_model = {"model": None}

# Training function using XGBoost
def train_model(_):
    output.clear_output(wait=True)
    with output:
        try:
            scaler_X = StandardScaler()
            scaler_Y = StandardScaler()
            X_scaled = scaler_X.fit_transform(X)
            Y_scaled = scaler_Y.fit_transform(Y)

            pca = PCA(n_components=pca_slider.value)
            Y_pca = pca.fit_transform(Y_scaled)

            X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_pca, test_size=0.3, random_state=random_state_slider.value)
            X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=random_state_slider.value)

            models = []
            for i in range(Y_train.shape[1]):
                model = xgb.XGBRegressor(n_estimators=trees_slider.value,
                                         learning_rate=lr_slider.value,
                                         objective='reg:squarederror',
                                         random_state=random_state_slider.value,
                                         verbosity=0)
                model.fit(X_train, Y_train[:, i])
                models.append(model)

            # Predictions and evaluation
            train_pred = np.column_stack([model.predict(X_train) for model in models])
            val_pred = np.column_stack([model.predict(X_val) for model in models])
            test_pred = np.column_stack([model.predict(X_test) for model in models])

            train_mse = mean_squared_error(Y_train, train_pred)
            val_mse = mean_squared_error(Y_val, val_pred)
            test_mse = mean_squared_error(Y_test, test_pred)

            train_r2 = r2_score(Y_train, train_pred)
            val_r2 = r2_score(Y_val, val_pred)
            test_r2 = r2_score(Y_test, test_pred)

            joblib.dump((models, pca, scaler_X, scaler_Y), "wsr_xgb_model.pkl")
            trained_model["model"] = (models, pca, scaler_X, scaler_Y)

            print(f"ðŸŒ² XGBoost Trained | Estimators={trees_slider.value} | LR={lr_slider.value} | PCA={pca_slider.value}")
            print(f"ðŸ“Š Data split: Train={len(X_train)} | Val={len(X_val)} | Test={len(X_test)}")
            print(f"\nTraining MSE: {train_mse:.5e} | RÂ²: {train_r2:.5f}")
            print(f"Validation MSE: {val_mse:.5e} | RÂ²: {val_r2:.5f}")
            print(f"Test MSE: {test_mse:.5e} | RÂ²: {test_r2:.5f}")
            print("ðŸ’¾ Model saved as 'wsr_xgb_model.pkl'")
        except Exception as e:
            print("Training failed:", e)

# Prediction function
def predict_species(_):
    with output:
        if trained_model["model"] is None:
            print("Please train the model first.")
            return
        try:
            time_val = time_input.value
            temp_val = temp_input.value
            input_vals = np.array([[time_val, temp_val]])

            models, pca, scaler_X, scaler_Y = trained_model["model"]
            x_scaled = scaler_X.transform(input_vals)
            y_pred_pca = np.column_stack([model.predict(x_scaled) for model in models])
            y_scaled = pca.inverse_transform(y_pred_pca)
            y_pred = scaler_Y.inverse_transform(y_scaled)

            top5_idx = np.argsort(y_pred[0])[-5:][::-1]

            display(Markdown("### Top 5 Species by Mole Fraction:"))
            for i in top5_idx:
                print(f"{species_names[i]}: {y_pred[0][i]:.6e}")
        except Exception as e:
            print("Prediction failed:", e)

# Widget bindings
train_button.on_click(train_model)
predict_button.on_click(predict_species)

slider_box = widgets.VBox([
    lr_slider, trees_slider, pca_slider, random_state_slider
])
input_box = widgets.VBox([time_input, temp_input])
button_box = widgets.HBox([train_button, predict_button])

display(slider_box, input_box, button_box, output)